{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from underthesea import word_tokenize\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning(raw_texts):\n",
    "    '''\n",
    "    Clean other punct, and other simple processs\n",
    "    '''\n",
    "    data = []\n",
    "    for sent in raw_texts:\n",
    "        sent = sent.replace('\\n','')\n",
    "        sent = sent.replace(':',',')\n",
    "        sent = sent.replace('!','.')\n",
    "        sent = sent.replace('?','.')\n",
    "        sent = sent.replace(';',',')\n",
    "        sent = sent.replace('\"','')\n",
    "        sent = sent.replace(')','')\n",
    "        sent = sent.replace('(','')\n",
    "        sent = sent.replace('“','')\n",
    "        sent = sent.replace('”','')\n",
    "        sent = sent.replace('-','')\n",
    "        sent = sent.replace('_','')\n",
    "        sent = sent.replace('+','')\n",
    "        sent = sent.replace('=','')\n",
    "        sent = sent.replace('[','')\n",
    "        sent = sent.replace(']','')\n",
    "        sent = sent.replace('{','')\n",
    "        sent = sent.replace('}','')\n",
    "        sent = sent.replace('*','')\n",
    "        sent = sent.replace('&','')\n",
    "        sent = sent.replace('^','')\n",
    "        sent = sent.replace('%','')\n",
    "        sent = sent.replace('$','')\n",
    "        sent = sent.replace('#','')\n",
    "        sent = sent.replace('@','')\n",
    "        sent = sent.replace('!','')\n",
    "        sent = sent.replace('`','')\n",
    "        sent = sent.replace('~','')\n",
    "        sent = sent.replace('/','')\n",
    "        sent = sent.replace('|','')\n",
    "        sent = sent.replace('…','')\n",
    "        sent = sent.replace(',.','')\n",
    "        if count_comma(sent) >= 2:\n",
    "            if count_word(sent) >= 20 and count_word(sent) <= 50:\n",
    "                sent = word_tokenize(sent)\n",
    "                sent = ' '.join(sent)\n",
    "                sent = sent+'\\n'\n",
    "                data.append(sent)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94f3c23290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Data_byADuc.txt', 'r') as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_comma(sent):\n",
    "    return sent.count(\",\")\n",
    "\n",
    "def count_word(string):\n",
    "    return(len(string.split()))\n",
    "\n",
    "def cleaning(raw_texts):\n",
    "    '''\n",
    "    Clean other punct, and other simple processs\n",
    "    '''\n",
    "    data = []\n",
    "    for sent in raw_texts:\n",
    "        sent = sent.replace('\\n','')\n",
    "        sent = sent.replace(':',',')\n",
    "        sent = sent.replace('!','.')\n",
    "        sent = sent.replace('?','.')\n",
    "        sent = sent.replace(';',',')\n",
    "        sent = sent.replace('\"','')\n",
    "        sent = sent.replace(')','')\n",
    "        sent = sent.replace('(','')\n",
    "        sent = sent.replace('“','')\n",
    "        sent = sent.replace('”','')\n",
    "        sent = sent.replace('-','')\n",
    "        sent = sent.replace('_','')\n",
    "        sent = sent.replace('+','')\n",
    "        sent = sent.replace('=','')\n",
    "        sent = sent.replace('[','')\n",
    "        sent = sent.replace(']','')\n",
    "        sent = sent.replace('{','')\n",
    "        sent = sent.replace('}','')\n",
    "        sent = sent.replace('*','')\n",
    "        sent = sent.replace('&','')\n",
    "        sent = sent.replace('^','')\n",
    "        sent = sent.replace('%','')\n",
    "        sent = sent.replace('$','')\n",
    "        sent = sent.replace('#','')\n",
    "        sent = sent.replace('@','')\n",
    "        sent = sent.replace('!','')\n",
    "        sent = sent.replace('`','')\n",
    "        sent = sent.replace('~','')\n",
    "        sent = sent.replace('/','')\n",
    "        sent = sent.replace('|','')\n",
    "        sent = sent.replace('…','')\n",
    "        sent = sent.replace(',.','')\n",
    "        if count_comma(sent) >= 2:\n",
    "            if count_word(sent) >= 20 and count_word(sent) <= 50:\n",
    "                sent = word_tokenize(sent)\n",
    "                sent = ' '.join(sent)\n",
    "                sent = sent+'\\n'\n",
    "                data.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def create_label(text):\n",
    "\n",
    "    '''\n",
    "    Take a string -> intext and label\n",
    "    '''\n",
    "    tokens = word_tokenize(text)\n",
    "    words = []\n",
    "    ids_punct = {',':[], '.':[]}\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        if token not in ids_punct.keys():\n",
    "            words.append(token)\n",
    "            i+=1\n",
    "        else:\n",
    "            ids_punct[token].append(i-1)\n",
    "\n",
    "    label = [0]*len(words)\n",
    "    for pun, ids in ids_punct.items():\n",
    "        for index in ids:\n",
    "            label[index] = 1 if pun == ',' else 2\n",
    "    \n",
    "    in_text = '<fff>'.join(words)\n",
    "    return in_text, label\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_train_data(RAW_PATH = './data/Data_byADuc.txt', IN_TEXT_PATH = './demo_data/text.txt', \n",
    "                             LABEL_PATH = './demo_data/label.txt'):\n",
    "    # start processing\n",
    "    with open(RAW_PATH, 'r') as f:\n",
    "        texts = f.read().splitlines()\n",
    "\n",
    "\n",
    "    texts = cleaning(texts)\n",
    "    texts, labels = [], []\n",
    "    for text in lines:\n",
    "        in_text, label = create_label(text)\n",
    "        texts.append(in_text)\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "    with open(IN_TEXT_PATH, 'w') as f:\n",
    "        for text in texts:\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(LABEL_PATH, 'w') as f:\n",
    "        for label in labels :\n",
    "            label = [str(ele) for ele in label]\n",
    "            label = ' '.join(label)\n",
    "            f.write(label)\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylines = lines[50000:51000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = cleaning(mylines)\n",
    "texts, labels = [], []\n",
    "for text in lines:\n",
    "    in_text, label = create_label(text)\n",
    "    texts.append(in_text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./demo_data/testtext.txt', 'w') as f:\n",
    "    for text in texts:\n",
    "        f.write(text)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('./demo_data/testlabel.txt', 'w') as f:\n",
    "    for label in labels :\n",
    "        label = [str(ele) for ele in label]\n",
    "        label = ' '.join(label)\n",
    "        f.write(label)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        \n",
    "        with open(self.data_path, 'r') as f:\n",
    "            sents = f.read().splitlines()\n",
    "        \n",
    "        \n",
    "        self.sents = [sent.split('<fff>') for sent in sents]\n",
    "        \n",
    "        self.word2id = dict()\n",
    "        i = 1\n",
    "        for sent in self.sents:\n",
    "            for word in sent:\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = i\n",
    "                    i+=1\n",
    "        self.word2id['<pad>'] = 0\n",
    "        self.id2word = {v:k for (k,v) in self.word2id.items()}\n",
    "        \n",
    "        self.vocab_size = len(self.word2id)\n",
    "        \n",
    "        self.sents = [[self.word2id[word] for word in sent] for sent in self.sents]\n",
    "        \n",
    "        self.sents = sequence.pad_sequences(self.sents, maxlen=32, padding=\"post\")\n",
    "                \n",
    "        \n",
    "        with open(self.label_path, 'r') as f:\n",
    "            labels = f.read().splitlines()\n",
    "        \n",
    "        self.labels = [list(map(int,label.split())) for label in labels]\n",
    "        self.labels = sequence.pad_sequences(self.labels, maxlen=32, padding=\"post\", value=3)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return {'data':self.sents[index], 'label':self.labels[index]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = MyDataset(data_path ='./demo_data/text.txt', label_path ='./demo_data/label.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with batchsize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_batch_iter(dataset, batch_size):\n",
    "    b_words = []\n",
    "    b_labels = []\n",
    "    for data in dataset:\n",
    "        b_words.append(data['data'])\n",
    "        b_labels.append(data['label'])\n",
    "        \n",
    "        if len(b_words) == batch_size:\n",
    "            yield {'data':np.array(b_words, dtype=int), 'label':np.array(b_labels, dtype=int)}\n",
    "            b_words, b_labels = [], []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[  3   4   5   6   7   8   9  10  11   6  12  13  14  15   8  16  17  18\n",
      "    4  19  20  21  22  23  24  25  26  27  28  29  30   6]\n",
      " [ 31   8  32  10  33  34  35  36  37  38  39  40   2  38  12  41  42  43\n",
      "   44  43  45  46  47  38  48  41  42  49   0   0   0   0]\n",
      " [ 50  51  52  53  45  54  55  56  57  16  58  59  60  61  45  62  63  17\n",
      "   64  65  66  67  48  68  69   0   0   0   0   0   0   0]\n",
      " [ 71  72  73  74  75  12  72  76  77  78  12  79  80  81  82  83  84  85\n",
      "   86  12  87  88  89  82  90  65  91  92  93  27  94  95]\n",
      " [ 97  98  99 100  45 101  20  65 102 103 104  33 105 106 107 108 109  98\n",
      "  110   8 111 106  31  10 112 113  45 114 115 116   8 117]]\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3 3 3]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3 3 3 3 3 3]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "for batch, data in enumerate(dataset_batch_iter(traindataset, 5)):\n",
    "    print(batch)\n",
    "    print(data['data'])\n",
    "    print(data['label'])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size,\n",
    "                 hidden_dim, n_layers):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=embedding_size)\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        prob = self.softmax(output)\n",
    "        \n",
    "        \n",
    "        return prob, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "       \n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataset, batch_size):\n",
    "    # training\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    train_loss = 0.\n",
    "    for batch, data in enumerate(dataset_batch_iter(train_dataset, batch_size)):\n",
    "        input_tensor = torch.Tensor(data['data']).type(torch.LongTensor)\n",
    "        target_tensor = torch.Tensor(data['label']).type(torch.LongTensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        loss = nll_loss(output.view(-1, num_categories), target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere(model, input_tensor):\n",
    "    # infering\n",
    "    model.eval()\n",
    "    batch_size = input_tensor.shape[0]\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    output, _ = model(input_tensor, hidden)\n",
    "    prediction = output.argmax(dim=-1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_dataset, test_dataset):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataset, batch_size)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, accuracy = test(model, test_dataset, batch_size)\n",
    "            print(f\"Epoch {epoch} --train loss {train_loss} -- test loss {test_loss}-- test acc {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore(tokens, punct):\n",
    "    id2word = dataset.id2word\n",
    "    convert = {0:'', 1:',', 2:'.', 3:''}\n",
    "    seq = [id2word[token]+convert[punct[i]] for i, token in enumerate(tokens)]\n",
    "    seq = ' '.join(seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --train loss 2916.7114140987396 -- test loss 2900.8115725517273-- test acc 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-38ba8551cac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-3551e53322ea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-4e60eaa08bb9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dataset, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(101, traindataset, traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, data in enumerate(dataset_batch_iter(dataset, 1)):\n",
    "    data, label = data['data'], data['label']\n",
    "    tokens = list(data[0])\n",
    "    label = list(label[0])\n",
    "    data = torch.Tensor(data).type(torch.LongTensor)\n",
    "    pred = infere(model, data)\n",
    "    pred = np.array(pred[0])\n",
    "    print(\"true sent:\")\n",
    "    print(restore(tokens, label))\n",
    "    print(\"--------\")\n",
    "    print(\"prediction:\")\n",
    "    print(restore(tokens, pred))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
