{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0c98032290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        \n",
    "        with open(self.data_path, 'r') as f:\n",
    "            sents = f.read().splitlines()\n",
    "        \n",
    "        \n",
    "        self.sents = [sent.split() for sent in sents]\n",
    "        \n",
    "        self.word2id = dict()\n",
    "        i = 1\n",
    "        for sent in self.sents:\n",
    "            for word in sent:\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = i\n",
    "                    i+=1\n",
    "        self.word2id['<pad>'] = 0\n",
    "        self.id2word = {v:k for (k,v) in self.word2id.items()}\n",
    "        \n",
    "        self.vocab_size = len(self.word2id)\n",
    "        \n",
    "        self.sents = [[self.word2id[word] for word in sent] for sent in self.sents]\n",
    "        \n",
    "        self.sents = sequence.pad_sequences(self.sents, maxlen=32, padding=\"post\")\n",
    "                \n",
    "        \n",
    "        with open(self.label_path, 'r') as f:\n",
    "            labels = f.read().splitlines()\n",
    "        \n",
    "        self.labels = [list(map(int,label.split())) for label in labels]\n",
    "        self.labels = sequence.pad_sequences(self.labels, maxlen=32, padding=\"post\", value=3)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return {'data':self.sents[index], 'label':self.labels[index]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = MyDataset(data_path ='./data/traindata.txt', label_path ='./data/trainlabel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617, 617)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/traindata.txt', 'r') as f:\n",
    "    traindata = f.read().splitlines()\n",
    "with open('./data/trainlabel.txt', 'r') as f:\n",
    "    trainlabel = f.read().splitlines()\n",
    "\n",
    "len(traindata), len(trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545, 545)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/testdata.txt', 'r') as f:\n",
    "    testdata = f.read().splitlines()\n",
    "with open('./data/testlabel.txt', 'r') as f:\n",
    "    testlabel = f.read().splitlines()\n",
    "\n",
    "len(testdata), len(testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = traindata + testdata\n",
    "label = trainlabel + testlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 1162)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/small_traintext.txt','w') as f:\n",
    "    for i in data[:1000]:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "with open('./data/small_trainlabel.txt','w') as f:\n",
    "    for i in label[:1000]:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/small_validtext.txt','w') as f:\n",
    "    for i in data[1000:]:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "with open('./data/small_validlabel.txt','w') as f:\n",
    "    for i in label[1000:]:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with batchsize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_batch_iter(dataset, batch_size):\n",
    "    b_words = []\n",
    "    b_labels = []\n",
    "    for data in dataset:\n",
    "        b_words.append(data['data'])\n",
    "        b_labels.append(data['label'])\n",
    "        \n",
    "        if len(b_words) == batch_size:\n",
    "            yield {'data':np.array(b_words, dtype=int), 'label':np.array(b_labels, dtype=int)}\n",
    "            b_words, b_labels = [], []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17  9 18  2 13 14 19 20\n",
      "  21 22 23 24 25  0  0  0]\n",
      " [26 27  4 28 29 30 31 32 33 34 35 36 37 38 39 40 30 41 42 43 44 45 36 44\n",
      "  46 45 47 48 49 50  0  0]]\n",
      "[[0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 3 3 3]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 2 3 3]]\n"
     ]
    }
   ],
   "source": [
    "for batch, data in enumerate(dataset_batch_iter(dataset, 2)):\n",
    "    print(batch)\n",
    "    print(data['data'])\n",
    "    print(data['label'])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size,\n",
    "                 hidden_dim, n_layers):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=embedding_size)\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        prob = self.softmax(output)\n",
    "        \n",
    "        \n",
    "        return prob, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "       \n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = dataset.vocab_size\n",
    "embedding_size = 16\n",
    "num_categories = 4\n",
    "hidden_dim = 16\n",
    "n_layers = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size, embedding_size, num_categories, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, batch_size = 1):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for batch, data in enumerate(dataset_batch_iter(test_dataset, batch_size)):\n",
    "        input_tensor = torch.Tensor(data['data']).type(torch.LongTensor)\n",
    "        target_tensor = torch.Tensor(data['label']).type(torch.LongTensor)\n",
    "    \n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        prediction = output.argmax(dim = -1)\n",
    "        \n",
    "        loss = nll_loss(output.view(-1, num_categories), target_tensor.view(-1))\n",
    "            \n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        accuracy = torch.sum(prediction == target_tensor)\n",
    "        accuracy = accuracy/(batch_size*length)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataset, batch_size):\n",
    "    # training\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    train_loss = 0.\n",
    "    for batch, data in enumerate(dataset_batch_iter(train_dataset, batch_size)):\n",
    "        input_tensor = torch.Tensor(data['data']).type(torch.LongTensor)\n",
    "        target_tensor = torch.Tensor(data['label']).type(torch.LongTensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        loss = nll_loss(output.view(-1, num_categories), target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere(model, input_tensor):\n",
    "    # infering\n",
    "    model.eval()\n",
    "    batch_size = input_tensor.shape[0]\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    output, _ = model(input_tensor, hidden)\n",
    "    prediction = output.argmax(dim=-1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_dataset, test_dataset):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataset, batch_size)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, accuracy = test(model, test_dataset, batch_size)\n",
    "            print(f\"Epoch {epoch} --train loss {train_loss} -- test loss {test_loss}-- test acc {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore(tokens, punct):\n",
    "    id2word = dataset.id2word\n",
    "    convert = {0:'', 1:',', 2:'.', 3:''}\n",
    "    seq = [id2word[token]+convert[punct[i]] for i, token in enumerate(tokens)]\n",
    "    seq = ' '.join(seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --train loss 993.0128664970398 -- test loss 971.1010429859161-- test acc 0.5625\n",
      "Epoch 10 --train loss 921.256861448288 -- test loss 917.9510951042175-- test acc 0.75\n",
      "Epoch 20 --train loss 892.2851436138153 -- test loss 889.129175901413-- test acc 0.796875\n",
      "Epoch 30 --train loss 870.1857359409332 -- test loss 868.4203298091888-- test acc 0.859375\n",
      "Epoch 40 --train loss 855.3676483631134 -- test loss 853.4807364940643-- test acc 0.875\n",
      "Epoch 50 --train loss 845.1314046382904 -- test loss 843.9620831012726-- test acc 0.890625\n",
      "Epoch 60 --train loss 837.943708896637 -- test loss 840.8105971813202-- test acc 0.921875\n",
      "Epoch 70 --train loss 834.7703297138214 -- test loss 832.6245355606079-- test acc 0.90625\n",
      "Epoch 80 --train loss 829.7893669605255 -- test loss 829.5500228404999-- test acc 0.96875\n",
      "Epoch 90 --train loss 827.6537466049194 -- test loss 828.8166174888611-- test acc 0.984375\n",
      "Epoch 100 --train loss 826.2723476886749 -- test loss 825.8633854389191-- test acc 0.984375\n"
     ]
    }
   ],
   "source": [
    "train(101, dataset, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true sent:\n",
      "do đó, khi các doanh_nghiệp tiếp_cận sự cải_tiến, họ có xu_hướng đưa những người tốt nhất của họ vào đó, những người đã thể_hiện sở_trường để nhận được kết_quả. <pad> <pad> <pad>\n",
      "--------\n",
      "prediction:\n",
      "do đó, khi các doanh_nghiệp tiếp_cận sự cải_tiến, họ có xu_hướng đưa những người tốt nhất của họ vào đó, những người đã thể_hiện sở_trường để nhận được kết_quả. <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for batch, data in enumerate(dataset_batch_iter(dataset, 1)):\n",
    "    data, label = data['data'], data['label']\n",
    "    tokens = list(data[0])\n",
    "    label = list(label[0])\n",
    "    data = torch.Tensor(data).type(torch.LongTensor)\n",
    "    pred = infere(model, data)\n",
    "    pred = np.array(pred[0])\n",
    "    print(\"true sent:\")\n",
    "    print(restore(tokens, label))\n",
    "    print(\"--------\")\n",
    "    print(\"prediction:\")\n",
    "    print(restore(tokens, pred))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
