{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff2e00902b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        \n",
    "        with open(self.data_path, 'r') as f:\n",
    "            sents = f.read().splitlines()\n",
    "        \n",
    "        tokenizer = text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(sents)    \n",
    "        self.word2id = tokenizer.word_index\n",
    "        self.word2id['<pad>'] = 0\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.vocab_size = len(self.word2id)\n",
    "        \n",
    "        print(self.word2id)\n",
    "        \n",
    "        self.sents = [sent.split() for sent in sents]\n",
    "        self.sents = [[self.word2id[word] for word in sent] for sent in self.sents]\n",
    "        \n",
    "        self.sents = sequence.pad_sequences(self.sents, maxlen=32, padding=\"post\")\n",
    "                \n",
    "        \n",
    "        with open(self.label_path, 'r') as f:\n",
    "            labels = f.read().splitlines()\n",
    "        \n",
    "        self.labels = [list(map(int,label.split())) for label in labels]\n",
    "        self.labels = sequence.pad_sequences(self.labels, maxlen=32, padding=\"post\", value=3)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return {'data':self.sents[index], 'label':self.labels[index]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'hygge': 2, 'you': 3, 'day': 4, 'like': 5, 'miss': 6, 'how': 7, 'is': 8, 'your': 9, 'good': 10, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(data_path ='./train.txt', label_path ='./label.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([1, 5, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'label': array([0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing with batchsize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_batch_iter(dataset, batch_size):\n",
    "    b_words = []\n",
    "    b_labels = []\n",
    "    for data in dataset:\n",
    "        b_words.append(data['data'])\n",
    "        b_labels.append(data['label'])\n",
    "        \n",
    "        if len(b_words) == batch_size:\n",
    "            yield {'data':np.array(b_words, dtype=int), 'label':np.array(b_labels, dtype=int)}\n",
    "            b_words, b_labels = [], []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [0 0 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "for batch, data in enumerate(dataset_batch_iter(dataset, 2)):\n",
    "    print(batch)\n",
    "    print(data['data'])\n",
    "    print(data['label'])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size,\n",
    "                 hidden_dim, n_layers):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=embedding_size)\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        prob = self.softmax(output)\n",
    "        \n",
    "        \n",
    "        return prob, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "       \n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = dataset.vocab_size\n",
    "embedding_size = 16\n",
    "num_categories = 4\n",
    "hidden_dim = 16\n",
    "n_layers = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size, embedding_size, num_categories, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, batch_size = 1):\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for batch, data in enumerate(dataset_batch_iter(test_dataset, batch_size)):\n",
    "        input_tensor = torch.Tensor(data['data']).type(torch.LongTensor)\n",
    "        target_tensor = torch.Tensor(data['label']).type(torch.LongTensor)\n",
    "    \n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        prediction = output.argmax(dim = -1)\n",
    "        \n",
    "        loss = nll_loss(output.view(-1, num_categories), target_tensor.view(-1))\n",
    "            \n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        accuracy = torch.sum(prediction == target_tensor)\n",
    "        accuracy = accuracy/(batch_size*length)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataset, batch_size):\n",
    "    # training\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    train_loss = 0.\n",
    "    for batch, data in enumerate(dataset_batch_iter(train_dataset, batch_size)):\n",
    "        input_tensor = torch.Tensor(data['data']).type(torch.LongTensor)\n",
    "        target_tensor = torch.Tensor(data['label']).type(torch.LongTensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "        hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        loss = nll_loss(output.view(-1, num_categories), target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere(model, input_tensor):\n",
    "    # infering\n",
    "    model.eval()\n",
    "    batch_size = input_tensor.shape[0]\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    output, _ = model(input_tensor, hidden)\n",
    "    prediction = output.argmax(dim=-1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_dataset, test_dataset):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataset, batch_size)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, accuracy = test(model, test_dataset, batch_size)\n",
    "            print(f\"Epoch {epoch} --train loss {train_loss} -- test loss {test_loss}-- test acc {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --train loss 5.948636054992676 -- test loss 5.948574542999268-- test acc 0.984375\n",
      "Epoch 10 --train loss 5.9481964111328125 -- test loss 5.948201894760132-- test acc 0.984375\n",
      "Epoch 20 --train loss 5.948175430297852 -- test loss 5.948178052902222-- test acc 0.984375\n",
      "Epoch 30 --train loss 5.948169946670532 -- test loss 5.948172092437744-- test acc 0.984375\n",
      "Epoch 40 --train loss 5.948162078857422 -- test loss 5.948161363601685-- test acc 0.984375\n",
      "Epoch 50 --train loss 5.948155641555786 -- test loss 5.948159217834473-- test acc 0.984375\n",
      "Epoch 60 --train loss 5.948153734207153 -- test loss 5.948153018951416-- test acc 0.984375\n",
      "Epoch 70 --train loss 5.9481518268585205 -- test loss 5.948151111602783-- test acc 0.984375\n",
      "Epoch 80 --train loss 5.948145389556885 -- test loss 5.948149681091309-- test acc 0.984375\n",
      "Epoch 90 --train loss 5.94814395904541 -- test loss 5.948143482208252-- test acc 0.984375\n",
      "Epoch 100 --train loss 5.948137998580933 -- test loss 5.948137521743774-- test acc 0.984375\n"
     ]
    }
   ],
   "source": [
    "train(101, dataset, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = infere(model, input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "         3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "         3, 3, 3, 3, 3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 1, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
